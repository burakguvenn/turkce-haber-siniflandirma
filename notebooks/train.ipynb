{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824185e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri kur\n",
    "!pip install -q tensorflow\n",
    "!pip install -q gradio\n",
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5e9db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLTK Stopwords indir\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Model Parametreleri\n",
    "MAX_WORDS = 15000\n",
    "MAX_LEN = 250\n",
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "\n",
    "print(\"Kutuphaneler yuklendi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ab19e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CSV dosyasını yükle\n",
    "from google.colab import files\n",
    "print(\"Lutfen haberler_top5.csv dosyasini yukleyin:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Veriyi oku\n",
    "df = pd.read_csv('haberler_top5.csv')\n",
    "\n",
    "# 'genel' kategorisini veri setinden çıkar\n",
    "df = df[df['category'] != 'genel']\n",
    "\n",
    "print(\"Kategori dagilimi:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Text Temizleme Fonksiyonu\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Küçük harf\n",
    "    text = re.sub(r'\\d+', '', text) # Sayıları kaldır\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Noktalama işaretlerini kaldır\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # Stopwords kaldır\n",
    "    return text\n",
    "\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "print(\"Veri on isleme tamamlandi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc889d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CSV dosyasını yükle\n",
    "from google.colab import files\n",
    "print(\"Lutfen haberler_top5.csv dosyasini yukleyin:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Veriyi oku\n",
    "df = pd.read_csv('haberler_top5.csv')\n",
    "\n",
    "# 'genel' kategorisini veri setinden çıkar\n",
    "df = df[df['category'] != 'genel']\n",
    "\n",
    "print(\"Kategori dagilimi:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Text Temizleme Fonksiyonu\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Küçük harf\n",
    "    text = re.sub(r'\\d+', '', text) # Sayıları kaldır\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Noktalama işaretlerini kaldır\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # Stopwords kaldır\n",
    "    return text\n",
    "\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "print(\"Veri on isleme tamamlandi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2842852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['text_clean'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text_clean'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Veri setleri hazirlandi.\")\n",
    "print(f\"Egitim boyutu: {X_train.shape}\")\n",
    "print(f\"Test boyutu: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea9ee9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_hybrid_model(num_classes):\n",
    "    inputs = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    # Embedding Layer\n",
    "    x = Embedding(MAX_WORDS, EMBEDDING_DIM)(inputs)\n",
    "    \n",
    "    # Branch 1: CNN (Feature Extraction)\n",
    "    x1 = Conv1D(128, 5, activation='relu')(x)\n",
    "    x1 = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    # Branch 2: LSTM (Sequence Modeling)\n",
    "    x2 = Bidirectional(LSTM(128))(x)\n",
    "    \n",
    "    # Concatenate\n",
    "    concat = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Dense Layers\n",
    "    x = Dense(128, activation='relu')(concat)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "model = build_hybrid_model(num_classes)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c100c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Early Stopping ile eğitim\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "print(\"Egitim tamamlandi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26fa31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tahminler\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Gercek')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Eğitim Grafikleri\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4962d5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Modeli ve araçları kaydet\n",
    "model.save('news_classifier_hybrid.keras')\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Dosyalar kaydedildi, indirme hazirlaniyor...\")\n",
    "\n",
    "# Dosyaları indir\n",
    "from google.colab import files\n",
    "files.download('news_classifier_hybrid.keras')\n",
    "files.download('tokenizer.pkl')\n",
    "files.download('label_encoder.pkl')\n",
    "files.download('confusion_matrix.png')\n",
    "files.download('training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7c876",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_category(text):\n",
    "    # Preprocess\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    # Tokenize\n",
    "    seq = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    # Predict\n",
    "    pred = model.predict(padded)\n",
    "    pred_class_index = np.argmax(pred)\n",
    "    confidence = float(np.max(pred))\n",
    "    \n",
    "    result = label_encoder.classes_[pred_class_index]\n",
    "    \n",
    "    return f\"Kategori: {result}\", f\"Guven Skoru: {confidence:.2f}\"\n",
    "\n",
    "# Arayüz\n",
    "demo = gr.Interface(\n",
    "    fn=predict_category,\n",
    "    inputs=gr.Textbox(lines=5, placeholder=\"Haber metnini buraya girin...\"),\n",
    "    outputs=[gr.Textbox(label=\"Tahmin\"), gr.Textbox(label=\"Skor\")],\n",
    "    title=\"Turkce Haber Siniflandirma (Hybrid Model)\",\n",
    "    description=\"Spor, Ekonomi, Dunya veya Guncel kategorisindeki haberi yapistirin.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
